{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNU3+sK/QK6JW/G3BMn0LBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/furk4neg3/Aerial-Object-Detection/blob/main/Aerial_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aerial Object Detection\n",
        "\n",
        "üêà In this project, I made AI models for aerial object detection task. One is\n",
        "CNN model created by me from scratch, and the other is fine tuned Fast RCNN model.\n",
        "\n",
        "üêà SkyFusion aerial object detection data is used, you can reach it inside the notebook.\n",
        "\n",
        "üêà This was the first PyTorch project I designed myself (I worked on AI projects before but they were on Tensorflow, I'm trying to learn PyTorch too right now), other than the ones done in courses, and object detection is a completely new topic for me, so I'm open to criticisms and improvements.\n",
        "\n",
        "üêà Result in Fast RCNN model is pretty good, which has loss of 0.8746. In my researchs, I found that loss between 0 to 1 is considered good, so I can say that this model was successful."
      ],
      "metadata": {
        "id": "B8YxkoUFX_q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà That's the torchvision version I used creating this notebook, to ensure that it will run well on your system too, there's installation."
      ],
      "metadata": {
        "id": "j5uVc88OYy8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.15.1"
      ],
      "metadata": {
        "id": "9lEjrqlR4i7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking the Data\n",
        "\n",
        "üêà As mentioned above, data is from Kaggle."
      ],
      "metadata": {
        "id": "BeEGB47HZIoj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fMEq346vvgQE"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "cXixKAIrzUVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "JYu9vqCizfC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "eU5H-0FuzlFu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "6nvc91erzm_E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d kailaspsudheer/tiny-object-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G5Z72llzojy",
        "outputId": "fcc73816-fbe7-4c2d-fcec-05182445e259"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kailaspsudheer/tiny-object-detection\n",
            "License(s): apache-2.0\n",
            "Downloading tiny-object-detection.zip to /content\n",
            " 99% 177M/179M [00:01<00:00, 193MB/s]\n",
            "100% 179M/179M [00:01<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip tiny-object-detection.zip"
      ],
      "metadata": {
        "id": "fWWJRhB_ztNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n"
      ],
      "metadata": {
        "id": "1jV59M-mZS0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Features of images were in coco format, so install pycocotools."
      ],
      "metadata": {
        "id": "5aiNrpyBdIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "metadata": {
        "id": "IPVvljEUzw4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Install necessary libraries"
      ],
      "metadata": {
        "id": "l6kAbyHBdmdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "p0kRS64P0Co-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Receiving Features Data"
      ],
      "metadata": {
        "id": "Bpb5qeuQdu1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SkyFusion_ann = open(r\"SkyFusion/train/_annotations.coco.json\")\n",
        "SkyFusion_COCO = json.load(SkyFusion_ann)\n",
        "SkyFusion_train = pd.DataFrame(SkyFusion_COCO['annotations'])\n",
        "\n",
        "SkyFusion_ann = open(r\"SkyFusion/test/_annotations.coco.json\")\n",
        "SkyFusion_COCO = json.load(SkyFusion_ann)\n",
        "SkyFusion_test = pd.DataFrame(SkyFusion_COCO['annotations'])\n",
        "\n",
        "SkyFusion_ann = open(r\"SkyFusion/valid/_annotations.coco.json\")\n",
        "SkyFusion_COCO = json.load(SkyFusion_ann)\n",
        "SkyFusion_val = pd.DataFrame(SkyFusion_COCO['annotations'])"
      ],
      "metadata": {
        "id": "CtiOsArM0J2Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Short Description of Features Data\n",
        "\n",
        "ü´í I will only talk about the columns that will be used.\n",
        "\n",
        "üêà First, image_id column shows which image the line is linked to (because there can be different number of boxes in images).\n",
        "\n",
        "üêà Then, category_id is the label column. There are 3 labels that which car, ship and plane. Those are represented as 1, 2 and 3.\n",
        "\n",
        "üêà Finally, there's bbox column. That's used for drawing border boxes around objects."
      ],
      "metadata": {
        "id": "eiqqphPtd7Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SkyFusion_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Yd7kiWVH0S0K",
        "outputId": "0dacffb4-5ae9-4750-b158-b3161530d495"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  image_id  category_id                   bbox   area  \\\n",
              "0   0         0            3    [259, 49, 4.8, 9.6]  46.08   \n",
              "1   1         0            3   [284, 630, 4.8, 8.8]  42.24   \n",
              "2   2         0            3     [281, 568, 4, 8.8]  35.20   \n",
              "3   3         0            3  [288, 570, 4.8, 10.4]  49.92   \n",
              "4   4         0            3   [303, 553, 4.8, 9.6]  46.08   \n",
              "\n",
              "                                        segmentation  iscrowd  \n",
              "0  [[264, 48.8, 259.2, 48.8, 259.2, 58.4, 264, 58...        0  \n",
              "1  [[288.8, 630.4, 284, 630.4, 284, 639.2, 288.8,...        0  \n",
              "2  [[284.8, 568, 280.8, 568, 280.8, 576.8, 284.8,...        0  \n",
              "3  [[292.8, 569.6, 288, 569.6, 288, 580, 292.8, 5...        0  \n",
              "4  [[308, 552.8, 303.2, 552.8, 303.2, 562.4, 308,...        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71c7fc8d-fdb9-4f30-be81-01d655b5573d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>category_id</th>\n",
              "      <th>bbox</th>\n",
              "      <th>area</th>\n",
              "      <th>segmentation</th>\n",
              "      <th>iscrowd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[259, 49, 4.8, 9.6]</td>\n",
              "      <td>46.08</td>\n",
              "      <td>[[264, 48.8, 259.2, 48.8, 259.2, 58.4, 264, 58...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[284, 630, 4.8, 8.8]</td>\n",
              "      <td>42.24</td>\n",
              "      <td>[[288.8, 630.4, 284, 630.4, 284, 639.2, 288.8,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[281, 568, 4, 8.8]</td>\n",
              "      <td>35.20</td>\n",
              "      <td>[[284.8, 568, 280.8, 568, 280.8, 576.8, 284.8,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[288, 570, 4.8, 10.4]</td>\n",
              "      <td>49.92</td>\n",
              "      <td>[[292.8, 569.6, 288, 569.6, 288, 580, 292.8, 5...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[303, 553, 4.8, 9.6]</td>\n",
              "      <td>46.08</td>\n",
              "      <td>[[308, 552.8, 303.2, 552.8, 303.2, 562.4, 308,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71c7fc8d-fdb9-4f30-be81-01d655b5573d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71c7fc8d-fdb9-4f30-be81-01d655b5573d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71c7fc8d-fdb9-4f30-be81-01d655b5573d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ead8374-5e6c-4629-a5ca-751cd0cc038f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ead8374-5e6c-4629-a5ca-751cd0cc038f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ead8374-5e6c-4629-a5ca-751cd0cc038f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "SkyFusion_train",
              "summary": "{\n  \"name\": \"SkyFusion_train\",\n  \"rows\": 43575,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12579,\n        \"min\": 0,\n        \"max\": 43574,\n        \"num_unique_values\": 43575,\n        \"samples\": [\n          37884,\n          7134,\n          40444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 599,\n        \"min\": 0,\n        \"max\": 2093,\n        \"num_unique_values\": 2094,\n        \"samples\": [\n          1712,\n          1344,\n          845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"area\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1051.818352261564,\n        \"min\": 2.56,\n        \"max\": 9212.0,\n        \"num_unique_values\": 2556,\n        \"samples\": [\n          1095.0,\n          5270.0,\n          836.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"segmentation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iscrowd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Drop unnecessary columns."
      ],
      "metadata": {
        "id": "5N-MCX5Ufnhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SkyFusion_train = SkyFusion_train.drop([\"area\", \"id\", \"iscrowd\", \"segmentation\"], axis=1)\n",
        "SkyFusion_test = SkyFusion_test.drop([\"area\", \"id\", \"iscrowd\", \"segmentation\"], axis=1)\n",
        "SkyFusion_val = SkyFusion_val.drop([\"area\", \"id\", \"iscrowd\", \"segmentation\"], axis=1)"
      ],
      "metadata": {
        "id": "CX6KGrsHOKZx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SkyFusion_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6oXIy4TuFAlS",
        "outputId": "a3fc825a-b5a1-4345-b9d3-b19e3eff36f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id  category_id                   bbox\n",
              "0         0            3    [259, 49, 4.8, 9.6]\n",
              "1         0            3   [284, 630, 4.8, 8.8]\n",
              "2         0            3     [281, 568, 4, 8.8]\n",
              "3         0            3  [288, 570, 4.8, 10.4]\n",
              "4         0            3   [303, 553, 4.8, 9.6]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d11871d-f8e5-4de9-8fc6-a8b91cf8a72e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>category_id</th>\n",
              "      <th>bbox</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[259, 49, 4.8, 9.6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[284, 630, 4.8, 8.8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[281, 568, 4, 8.8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[288, 570, 4.8, 10.4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[303, 553, 4.8, 9.6]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d11871d-f8e5-4de9-8fc6-a8b91cf8a72e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d11871d-f8e5-4de9-8fc6-a8b91cf8a72e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d11871d-f8e5-4de9-8fc6-a8b91cf8a72e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-09d2f6ce-959d-48e4-98d9-11bc7c119c7a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09d2f6ce-959d-48e4-98d9-11bc7c119c7a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-09d2f6ce-959d-48e4-98d9-11bc7c119c7a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "SkyFusion_train",
              "summary": "{\n  \"name\": \"SkyFusion_train\",\n  \"rows\": 43575,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 599,\n        \"min\": 0,\n        \"max\": 2093,\n        \"num_unique_values\": 2094,\n        \"samples\": [\n          1712,\n          1344,\n          845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Check labels."
      ],
      "metadata": {
        "id": "TBXEKV5qf85d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts_A = SkyFusion_train['category_id'].value_counts()\n",
        "print(\"Counts of unique values of labels:\")\n",
        "print(value_counts_A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1xSm-srvnhE",
        "outputId": "4fedb8a3-8913-4e20-b548-7528da57e606"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of unique values of labels:\n",
            "category_id\n",
            "3    33396\n",
            "1     8696\n",
            "2     1483\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Receiving Images Data\n",
        "\n",
        "üêà Assigning images to variables for later use."
      ],
      "metadata": {
        "id": "-4Jj1b88gBmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Base directory\n",
        "base_dir = \"SkyFusion\"\n",
        "\n",
        "# Directories for train, test, and valid\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "valid_dir = os.path.join(base_dir, \"valid\")\n",
        "\n",
        "# Function to load images from a given directory\n",
        "def load_images(directory):\n",
        "    images = []\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "    # Traverse the directory and load image files\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(valid_extensions):\n",
        "                img_path = os.path.join(root, file)\n",
        "                img = Image.open(img_path).convert(\"RGB\")  # Load and convert to RGB\n",
        "                images.append(img)\n",
        "    return images\n",
        "\n",
        "# Load images from train, test, and valid directories\n",
        "train_images = load_images(train_dir)\n",
        "test_images = load_images(test_dir)\n",
        "valid_images = load_images(valid_dir)\n",
        "\n",
        "# Convert to NumPy arrays for model input\n",
        "train_images_np = [np.array(img) for img in train_images]\n",
        "test_images_np = [np.array(img) for img in test_images]\n",
        "valid_images_np = [np.array(img) for img in valid_images]\n",
        "\n",
        "# Print to see distribution between train, test and validation\n",
        "print(f\"Loaded {len(train_images_np)} train images, {len(test_images_np)} test images, and {len(valid_images_np)} valid images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_hgLYFnHKbY",
        "outputId": "adce6984-7c58-48c6-f713-071ea9fcce1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2094 train images, 449 test images, and 449 valid images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching Features with Images\n",
        "\n",
        "üêà As mentioned before, the number of boxes may vary depending on the images. So we should match features with images using image_id column. The function below does exactly that."
      ],
      "metadata": {
        "id": "vkzwg7wIgpuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_bboxes_with_images(df):\n",
        "  boxes_dict = {}\n",
        "\n",
        "  for _, row in df.iterrows():\n",
        "      image_id = row['image_id']\n",
        "      bbox = row['bbox']\n",
        "      # Turn bboxes into [x_min, y_min, x_max, y_max] format\n",
        "      x_min, y_min, width, height = bbox\n",
        "      x_max = x_min + width\n",
        "      y_max = y_min + height\n",
        "\n",
        "      if image_id not in boxes_dict:\n",
        "          boxes_dict[image_id] = []\n",
        "      boxes_dict[image_id].append([x_min, y_min, x_max, y_max, row['category_id']])  # [x_min, y_min, x_max, y_max, class]\n",
        "\n",
        "  images = []\n",
        "  boxes = []\n",
        "\n",
        "  for image_id in df['image_id'].unique():\n",
        "      if image_id < len(train_images_np):\n",
        "          images.append(train_images_np[image_id])\n",
        "          boxes.append(boxes_dict.get(image_id, []))\n",
        "\n",
        "  return images, boxes"
      ],
      "metadata": {
        "id": "yKF6hjFnFQ38"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, test and validation boxes\n",
        "train_images, train_boxes = match_bboxes_with_images(SkyFusion_train)\n",
        "test_images, test_boxes = match_bboxes_with_images(SkyFusion_test)\n",
        "val_images, val_boxes = match_bboxes_with_images(SkyFusion_val)"
      ],
      "metadata": {
        "id": "Ab7lIFthGPWa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if image and box counts match\n",
        "len(train_boxes), len(train_images), len(test_boxes), len(test_images), len(val_boxes), len(val_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKJzDj80QdQa",
        "outputId": "458ccf7f-7a0f-4847-8aa3-16df11c6e211"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2094, 2094, 449, 449, 449, 449)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Because there may be a different number of boxes in an image, the length of the elements in the boxes will also vary."
      ],
      "metadata": {
        "id": "zWo3NnAEiga2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of boxes\n",
        "test_boxes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cUWNTNQHNw8",
        "outputId": "574f1ef1-6e8c-4fa1-82f4-dad88199cc57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[411, 566, 429.4, 576.4, 2],\n",
              " [199, 546, 211, 554, 2],\n",
              " [205, 532, 217.8, 537.6, 2],\n",
              " [330, 554, 341.2, 570, 2],\n",
              " [242, 566, 252.4, 572.4, 2],\n",
              " [202, 540, 213.2, 544.8, 2],\n",
              " [198, 557, 206, 561.8, 2],\n",
              " [320, 554, 333.6, 571.6, 2],\n",
              " [313, 552, 325, 568.8, 2],\n",
              " [240, 534, 249.6, 540.4, 2],\n",
              " [204, 537, 214.4, 540.2, 2],\n",
              " [226, 570, 237.2, 579.6, 2],\n",
              " [198, 553, 206, 557.8, 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_boxes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cUCLEi2HP6J",
        "outputId": "8eb13a38-e9ce-4c90-ff26-a84df69160e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[256, 131, 289.5, 162, 1],\n",
              " [261, 93, 299, 121, 1],\n",
              " [275, 53, 314, 83.5, 1],\n",
              " [300, 16, 338.5, 45, 1],\n",
              " [185, 5, 217.5, 34.5, 1],\n",
              " [165, 46, 197, 88, 1],\n",
              " [123, 38, 152.5, 76.5, 1],\n",
              " [87, 22, 117, 57.5, 1],\n",
              " [607, 188, 638, 220, 1],\n",
              " [529, 150, 569.5, 178, 1],\n",
              " [511, 98, 559, 138.5, 1],\n",
              " [444, 60, 475, 101.5, 1],\n",
              " [411, 95, 446.5, 132.5, 1],\n",
              " [520, 293, 550, 330, 1],\n",
              " [483, 285, 506.5, 321.5, 1],\n",
              " [373, 281, 403.5, 312, 1],\n",
              " [363, 380, 400, 419.5, 1],\n",
              " [450, 562, 489.5, 604, 1],\n",
              " [370, 511, 391.5, 543.5, 1],\n",
              " [325, 478, 355, 520, 1],\n",
              " [280, 453, 311.5, 499.5, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Images to Model\n",
        "\n",
        "üêà Creating transformation for images. Because the pictures are taken from far away, objects look small. So I chose a big size of (640, 640) to be able to detect small looking objects."
      ],
      "metadata": {
        "id": "zmxvOoKni8Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Code below normalizes border box coordinates. This can improve the performance but\n",
        "# that was not the case for me, so I commented this out.\n",
        "\"\"\"def transform_bounding_boxes(boxes, size):\n",
        "    # Adjust bounding boxes if resizing the image\n",
        "    # size is (width, height) of the original image, and the new size is fixed.\n",
        "    # This assumes you're resizing to (640, 640) for example.\n",
        "    new_width, new_height = 640, 640\n",
        "    width_ratio = new_width / size[0]\n",
        "    height_ratio = new_height / size[1]\n",
        "\n",
        "    transformed_boxes = []\n",
        "    for box in boxes:\n",
        "        x_min, y_min, x_max, y_max, category_id = box\n",
        "        x_min *= width_ratio\n",
        "        x_max *= width_ratio\n",
        "        y_min *= height_ratio\n",
        "        y_max *= height_ratio\n",
        "        transformed_boxes.append([x_min, y_min, x_max, y_max, category_id])\n",
        "\n",
        "    return transformed_boxes\"\"\""
      ],
      "metadata": {
        "id": "YTTVlINxS4RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Datasets"
      ],
      "metadata": {
        "id": "j_b9yq6ej0Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, images, boxes, transform=None):\n",
        "        self.images = images\n",
        "        self.boxes = boxes\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        boxes = self.boxes[idx]\n",
        "\n",
        "        # Store the original size for bounding box transformation\n",
        "        original_size = (image.shape[1], image.shape[0])  # (width, height)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply image transformations\n",
        "\n",
        "        return image, torch.tensor(boxes, dtype=torch.float32)  # Return the transformed image and boxes\n",
        "\n",
        "# Create the dataset instance with transformations\n",
        "train_dataset = CustomObjectDetectionDataset(train_images, train_boxes, transform=image_transforms)\n",
        "test_dataset = CustomObjectDetectionDataset(test_images, test_boxes, transform=image_transforms)\n",
        "val_dataset = CustomObjectDetectionDataset(val_images, val_boxes, transform=image_transforms)"
      ],
      "metadata": {
        "id": "RRCCP20bS86l"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Check if dataset is shaped correctly."
      ],
      "metadata": {
        "id": "IDszF3I7klYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqi3OmXFJyLb",
        "outputId": "d2f429bd-3d42-41b7-ee9b-41354b2195f6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 640, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SEvklulKA0T",
        "outputId": "74e6ecdb-5e1d-4332-a66b-3df18cbc80b8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataloader\n",
        "\n",
        "üêà Zipping features and images inside dataloader. Batch size can change."
      ],
      "metadata": {
        "id": "aLrk3oCEkuQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set your batch size\n",
        "batch_size = 64  # Adjust based on your memory constraints\n",
        "\n",
        "# Create the DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "RleV3TyCPDnu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP7mygFKMVOg",
        "outputId": "c3e04f41-82ee-4f3e-ddd1-c8d5d0ae6ecf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üêà Checking shapes again."
      ],
      "metadata": {
        "id": "b23-uiSSlAG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_images_batch[0].shape, train_labels_batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiRnmfC-MXcz",
        "outputId": "23a0aefb-f829-48e1-e0fe-a51d60ab52a2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 640, 640]), torch.Size([14, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Model (Custom CNN Model)\n",
        "\n",
        "ü´í A simple CNN model created from scratch by me. Normally, I know that fine tuning existing model is a better choice, but I wanted to create a model from scratch too.\n",
        "\n",
        "üêà Backbone model is created from convolutional layers. After that, there's 2 heads:\n",
        "\n",
        "üêà First head is box_head. That outputs a box that contains the object that have been detected by the model.\n",
        "\n",
        "üêà Other head is class_head. This one outputs class of the object. Because this one makes classification, I decided to use linear layers too, and it gives better resutls than pure CNN."
      ],
      "metadata": {
        "id": "zN0yZRl9lGNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomObjectDetector(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomObjectDetector, self).__init__()\n",
        "        # Feature Extractor (simple CNN)\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # Output: (16, 320, 320)\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # Output: (32, 160, 160)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # Output: (64, 80, 80)\n",
        "        )\n",
        "\n",
        "        # Bounding Box Regressor\n",
        "        self.box_head = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 4, kernel_size=1),  # 4 coordinates for each box\n",
        "        )\n",
        "\n",
        "        # Classification Head with Dense Layers\n",
        "        self.class_head = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Added additional conv layer\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Reduce each feature map to a single value\n",
        "            nn.Flatten(),  # Flatten the output for the dense layer\n",
        "            nn.Linear(256, 128),  # First dense layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)  # Final dense layer outputting class scores\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Check input shape\n",
        "        if x.ndim != 4 or x.shape[1] != 3:\n",
        "            raise ValueError(f\"Expected input of shape (batch_size, 3, H, W), got {x.shape}\")\n",
        "\n",
        "        # Pass input through the feature extractor\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Predict bounding boxes and class scores\n",
        "        boxes = self.box_head(features)  # Shape: (batch_size, 4, height, width)\n",
        "        class_scores = self.class_head(features)  # Shape: (batch_size, num_classes, height, width)\n",
        "\n",
        "        return boxes, class_scores"
      ],
      "metadata": {
        "id": "UEOX8JXzJ-3i"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "üêà Had to use a custom loss function here because model gives 2 different outputs. For boxes, I use smooth L1 loss and for classes, cross entropy loss."
      ],
      "metadata": {
        "id": "lWHKOjPlmY5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(pred_boxes, true_boxes, pred_classes, true_classes):\n",
        "    # Box regression loss (smooth L1)\n",
        "    box_loss = F.smooth_l1_loss(pred_boxes, true_boxes)\n",
        "\n",
        "    # Classification loss (Cross Entropy)\n",
        "    class_loss = F.cross_entropy(pred_classes, true_classes)\n",
        "\n",
        "    total_loss = box_loss + class_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "075VyB2QH95X"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the First Model\n",
        "\n",
        "üêà This block of code was a real challenge for me, that's why it's that long. Had to do so many things to make it work. Tried to explain it well with comment lines, I recommend you to review the codes."
      ],
      "metadata": {
        "id": "xAweQFXEmrQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "\n",
        "def compute_loss(pred_boxes, true_boxes, pred_classes, true_classes):\n",
        "    # Flatten the predicted boxes\n",
        "    pred_boxes = pred_boxes.view(-1, 4)  # Shape: [batch_size * height * width, 4] 4 here is len([x_min, y_min, x_max, y_max])\n",
        "\n",
        "    # Check shape of true_boxes and handle padding if necessary\n",
        "    if true_boxes.shape[0] == 0:\n",
        "        raise ValueError(\"No true boxes available for loss computation.\")\n",
        "\n",
        "    # Get the number of boxes in the batch\n",
        "    num_boxes = true_boxes.size(0)\n",
        "\n",
        "    max_boxes = 100  # Maximum number of boxes we expect in any image\n",
        "    padded_true_boxes = torch.zeros((max_boxes, 4), device=pred_boxes.device)  # create a zero tensor for padding\n",
        "\n",
        "    # Fill with true boxes (assuming it's not exceeding max_boxes)\n",
        "    padded_true_boxes[:num_boxes, :] = true_boxes[:max_boxes, :]\n",
        "\n",
        "    # Compute Smooth L1 loss for bounding boxes\n",
        "    box_loss = F.smooth_l1_loss(pred_boxes[:num_boxes], padded_true_boxes[:num_boxes])\n",
        "\n",
        "    # For classification, ensure pred_classes is properly shaped\n",
        "    pred_classes = pred_classes.view(-1, pred_classes.shape[1])  # Flatten to [batch_size * height * width, num_classes]\n",
        "\n",
        "    # Use true_classes in a similar way, making sure they match the correct number of predictions\n",
        "    true_classes = true_classes.view(-1)  # Flatten to [total_boxes]\n",
        "\n",
        "    # Check for out-of-bounds classes\n",
        "    if true_classes.max() >= pred_classes.shape[1]:\n",
        "        raise ValueError(f\"True class index {true_classes.max()} is out of bounds for the number of classes {pred_classes.shape[1]}.\")\n",
        "\n",
        "    # Compute Cross-Entropy loss for classes\n",
        "    class_loss = F.cross_entropy(pred_classes[:num_boxes], true_classes[:num_boxes])\n",
        "\n",
        "    # Combine the losses\n",
        "    total_loss = box_loss + class_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "model = CustomObjectDetector(num_classes=3)  # We have 3 classes\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # On my experiments I decided that best performing lr is this.\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.8) # Because starting lr is a big value, I want to reduce it while model trains\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    start_time = time.time() # I want to see how much does it take to train the model\n",
        "\n",
        "    for data in train_dataloader:\n",
        "        images = torch.stack([img for img in data[0]])  # Stack images into a single tensor\n",
        "        targets = data[1]\n",
        "\n",
        "        # Predict bounding boxes and class scores\n",
        "        pred_boxes, pred_classes = model(images)\n",
        "\n",
        "        # Prepare true_boxes and true_classes for each image in the batch\n",
        "        true_boxes = []\n",
        "        true_classes = []\n",
        "\n",
        "        for i in range(images.size(0)):  # Loop through each image in the batch\n",
        "            # Extract the box and class for each image\n",
        "            box = targets[i][0][:4]  # Extracting the first box (coordinates)\n",
        "            cls = targets[i][0][4]   # Extracting the class label\n",
        "\n",
        "            true_boxes.append(box)\n",
        "            true_classes.append(int(cls))  # Ensure cls is an integer and append to the list\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        true_boxes = torch.stack(true_boxes).to(images.device)  # Ensure the boxes are on the same device\n",
        "\n",
        "        # Convert true_classes to tensor and ensure it's a long tensor\n",
        "        true_classes = torch.tensor(true_classes, dtype=torch.long, device=images.device)  # Ensure class labels are on the same device\n",
        "\n",
        "        true_classes -= 1  # Classes are in format 1, 2, 3 but we want them to be 0 indexed, so substract 1\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_loss(pred_boxes, true_boxes, pred_classes, true_classes)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    end_time = time.time()  # Record the end time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Calculate the duration\n",
        "    epoch_duration = epoch_duration / 60\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Duration: {epoch_duration:.2f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RbD1Hw7sfZK",
        "outputId": "d52148c4-5d10-4ee4-ad16-7de748344550"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 181.4098, LR: 0.008000, Duration: 3.49 minutes\n",
            "Epoch [2/10], Loss: 178.6821, LR: 0.006400, Duration: 3.54 minutes\n",
            "Epoch [3/10], Loss: 172.1516, LR: 0.005120, Duration: 3.52 minutes\n",
            "Epoch [4/10], Loss: 158.2260, LR: 0.004096, Duration: 3.50 minutes\n",
            "Epoch [5/10], Loss: 167.4919, LR: 0.003277, Duration: 3.53 minutes\n",
            "Epoch [6/10], Loss: 180.1522, LR: 0.002621, Duration: 3.54 minutes\n",
            "Epoch [7/10], Loss: 186.1154, LR: 0.002097, Duration: 3.55 minutes\n",
            "Epoch [8/10], Loss: 173.8505, LR: 0.001678, Duration: 3.55 minutes\n",
            "Epoch [9/10], Loss: 181.7435, LR: 0.001342, Duration: 3.57 minutes\n",
            "Epoch [10/10], Loss: 178.8379, LR: 0.001074, Duration: 3.58 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset and Dataloader for Fine Tuned Fast RCNN Model\n",
        "\n",
        "üêà Preparing datasets and dataloaders for fast rcnn model."
      ],
      "metadata": {
        "id": "8lkIlXwMp2ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CustomObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, images, boxes):\n",
        "        self.images = images  # NumPy arrays of shape (N, H, W, C)\n",
        "        self.boxes = boxes  # List of arrays with shape (num_boxes, 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img = img / 255.0  # Normalize to [0, 1]\n",
        "        img = img.transpose((2, 0, 1))  # Change to (C, H, W)\n",
        "\n",
        "        # Load bounding boxes and classes\n",
        "        boxes = np.array(self.boxes[idx])  # Ensure boxes is a NumPy array\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes[:, :4], dtype=torch.float32),  # x1, y1, x2, y2\n",
        "            \"labels\": torch.tensor(boxes[:, 4] - 1, dtype=torch.int64)  # class labels\n",
        "        }\n",
        "\n",
        "        return torch.tensor(img, dtype=torch.float32), target\n"
      ],
      "metadata": {
        "id": "tDr4iX0-P9oX"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "finetune_train_dataset = CustomObjectDetectionDataset(train_images, train_boxes)\n",
        "finetune_val_dataset = CustomObjectDetectionDataset(val_images, val_boxes)\n",
        "finetune_test_dataset = CustomObjectDetectionDataset(test_images, test_boxes)\n",
        "\n",
        "train_loader = DataLoader(finetune_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(finetune_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(finetune_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "UZK-GwJQeRtk"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Fast RCNN Model"
      ],
      "metadata": {
        "id": "bfm9bMETqJgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "\n",
        "num_classes = 3\n",
        "\n",
        "# Load a pre-trained Faster R-CNN model\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one (with num_classes)\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "1I0ILkPeeb1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627103ac-12a9-42e9-c31e-af0ab106dd85"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDMYy4mesfP6",
        "outputId": "b3349588-de24-483e-b948-28c0b3aa8670"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Fast RCNN Model\n",
        "\n",
        "üêà Normally, code below doesn't (shouldn't) have any problem. When I run it, it runs without problems for ~10 minutes, then I lose access to device on cloud. I searched it, and it says it's because I'm using too much processing power. I have Colab Pro and I'm using TPU with 300+ GB RAM, I don't understand how it's able to run this model normally but not the fine tuned version."
      ],
      "metadata": {
        "id": "3D9zBJwqqZcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import torch\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Number of classes (including background)\n",
        "num_classes = 4  # For example, we have 3 classes + background\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Define a new custom classifier head with additional layers\n",
        "class CustomPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CustomPredictor, self).__init__()\n",
        "        # Define additional layers\n",
        "        self.fc1 = torch.nn.Linear(in_channels, 512)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(512, 256)\n",
        "        self.fc3 = torch.nn.Linear(256, num_classes)  # Final classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Replace the classifier head with the custom predictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "class CustomRCNNHead(torch.nn.Module):\n",
        "    def __init__(self, base_predictor, num_classes):\n",
        "        super(CustomRCNNHead, self).__init__()\n",
        "        self.base_predictor = base_predictor\n",
        "        # Add more custom layers here\n",
        "        self.additional_fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(num_classes, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_predictor(x)\n",
        "\n",
        "        # x is a tuple of (class_logits, box_regression)\n",
        "        class_logits = x[0]\n",
        "\n",
        "        # Apply additional layers only to class logits\n",
        "        class_logits = self.additional_fc(class_logits)\n",
        "\n",
        "        # Return the modified logits and the original box regression\n",
        "        return class_logits, x[1]\n",
        "\n",
        "# Replace the existing predictor with this custom model\n",
        "model.roi_heads.box_predictor = CustomRCNNHead(model.roi_heads.box_predictor, num_classes)\"\"\""
      ],
      "metadata": {
        "id": "GyTcx99uNt3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Fast RCNN Model\n",
        "\n",
        "üêà 1 epoch takes more than an hour, so I couldn't train it for more than 1 epoch. Ideal condition is when you train it for more epochs.\n",
        "\n",
        "üêà As you can see, that model gives a way WAY better result. As in my researchs I found that loss between 0 to 1 is considered good, so our result is nice even in first epoch. On top of that, because objects are small in our images, it's even harder to achieve an acceptable loss value, so this result is really good."
      ],
      "metadata": {
        "id": "NTWVz3x4rahr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 1  # Set this based on your needs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    end_time = time.time()  # Record the end time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Calculate the duration\n",
        "    epoch_duration = epoch_duration / 60\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item():.4f},  Duration: {epoch_duration:.2f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb59ESNcehA4",
        "outputId": "1bbfe03b-8f39-4c56-995d-d996a864c801"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.8746,  Duration: 77.64 minutes\n"
          ]
        }
      ]
    }
  ]
}